1a)


<img width="806" alt="Screen Shot 2022-10-16 at 11 15 29 PM" src="https://user-images.githubusercontent.com/113271915/196094704-96be7a9d-3854-4feb-b61c-42202b00003e.png">


1b) The bias is included in the step function, where we defined the 
the threshold. There are only two real features; 1 and 0

1c) If the data point is classified correctly, the weight will be greater
vs. if it is classified incorrectly

2a) the loss function is the MSE

def criterion(yhat,y):                 
  return torch.mean((yhat-y)**2)

the gradient descent 

w.data = w.data - lr * w.grad.data <br>
b.data = b.data - lr * b.grad.data <br>

2b) the learning rule 

LOSS = [] <br>
lr = 0.01 <br>
w = torch.tensor(0., requires_grad=True) <br>
b = torch.tensor(-0.5, requires_grad=True) <br>
x_traint = torch.tensor(x_train) <br>
y_traint = torch.tensor(y_train) <br>

2c)
nn.Linear: #applies a linear transformation to incoming data - one layer <br>
nn.MSELoss: #measures the mean squared loss to the incoming data as loss function <br>
torch.optim.SGD: #an optimizer that takes the partial gradient based on one data sample at a time <br>
loss.backward: #finds gradient of loss wrt paremeters <br>
optimizer.step. #takes an optimization step <br>

2d. SGD is more commonly used in machine learning. Instead of a gradient based on 
all the samples, the computer calculates the loss based on one (or a few) samples at a time

2e. 

<img width="728" alt="Screen Shot 2022-10-04 at 4 40 43 PM" src="https://user-images.githubusercontent.com/113271915/196094201-bc8b85e5-520b-4ac1-8182-e428a615026d.png">

3)
![Screen Shot 2022-10-17 at 6 56 14 PM](https://user-images.githubusercontent.com/113271915/196310642-2de5f90c-4a90-439e-b1e8-18bdc5878762.png)


